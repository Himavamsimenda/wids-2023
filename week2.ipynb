{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e54ae2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3450cbe8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Arm:\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "    \n",
    "    def pull(self):\n",
    "        return np.random.binomial(1, self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceb40ba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MultiBandit:\n",
    "    def __init__(self, probs=[0.1, 0.2, 0.7, 0.4]):\n",
    "        self.__arms = [Arm(p) for p in probs]\n",
    "        self.__regret = 0\n",
    "        self.__maxp = max(probs)\n",
    "\n",
    "    def num_arms(self):\n",
    "        return len(self.__arms)\n",
    "\n",
    "    def pull(self, arm_num):\n",
    "        reward = self.__arms[arm_num].pull()\n",
    "        self.__regret += self.__maxp - self.__arms[arm_num].p\n",
    "        return reward\n",
    "    \n",
    "    def regret(self):\n",
    "        return self.__regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0813d9c6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class epsilongreedyalgorithm:\n",
    "    def __init__(self, num_arms, horizon, epsilon):\n",
    "        self.num_arms = num_arms\n",
    "        self.horizon = horizon\n",
    "        self.epsilon = epsilon\n",
    "        self.timestep = 0\n",
    "        self.arm_pulls = np.zeros(num_arms)\n",
    "        self.arm_rewards = np.zeros(num_arms)\n",
    "        self.regrets = np.zeros(horizon)\n",
    "        self.best_arm = None\n",
    "\n",
    "    def give_best_arm(self):\n",
    "        return self.best_arm\n",
    "\n",
    "    def select_arm(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "           \n",
    "            return np.random.choice(self.num_arms)\n",
    "        else:\n",
    "          \n",
    "            return np.argmax(self.arm_rewards / (self.arm_pulls + 1e-5))\n",
    "\n",
    "    def run_algorithm(self, bandit):\n",
    "        for _ in range(self.horizon):\n",
    "            arm_to_pull = self.select_arm()\n",
    "            reward = bandit.pull(arm_to_pull)\n",
    "            self.arm_pulls[arm_to_pull] += 1\n",
    "            self.arm_rewards[arm_to_pull] += reward\n",
    "            self.timestep += 1\n",
    "            self.regrets[_] = bandit.regret()\n",
    "\n",
    "        self.best_arm = np.argmax(self.arm_rewards / (self.arm_pulls + 1e-5))\n",
    "\n",
    "    def plot(self, label):\n",
    "        plt.plot(np.cumsum(self.regrets), label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528711f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ucbalgorithm:\n",
    "    def __init__(self, num_arms, horizon, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.horizon = horizon\n",
    "        self.c = c  # Exploration-exploitation tradeoff parameter\n",
    "        self.timestep = 0\n",
    "        self.arm_pulls = np.zeros(num_arms)\n",
    "        self.arm_rewards = np.zeros(num_arms)\n",
    "        self.regrets = np.zeros(horizon)\n",
    "        self.best_arm = None\n",
    "\n",
    "    def give_best_arm(self):\n",
    "        return self.best_arm\n",
    "\n",
    "    def select_arm(self):\n",
    "       \n",
    "        if self.timestep < self.num_arms:\n",
    "            return self.timestep\n",
    "        ucb_values = self.arm_rewards / (self.arm_pulls + 1e-5) + self.c * np.sqrt(np.log(self.timestep) / (self.arm_pulls + 1e-5))\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def run_algorithm(self, bandit):\n",
    "        for _ in range(self.horizon):\n",
    "            arm_to_pull = self.select_arm()\n",
    "            reward = bandit.pull(arm_to_pull)\n",
    "            self.arm_pulls[arm_to_pull] += 1\n",
    "            self.arm_rewards[arm_to_pull] += reward\n",
    "            self.timestep += 1\n",
    "            self.regrets[_] = bandit.regret()\n",
    "\n",
    "        self.best_arm = np.argmax(self.arm_rewards / (self.arm_pulls + 1e-5))\n",
    "\n",
    "    def plot(self, label):\n",
    "        plt.plot(np.cumsum(self.regrets), label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f4772",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class thompsonsamplingalgorithm:\n",
    "    def __init__(self, num_arms, horizon):\n",
    "        self.num_arms = num_arms\n",
    "        self.horizon = horizon\n",
    "        self.timestep = 0\n",
    "        self.arm_successes = np.ones(num_arms)\n",
    "        self.arm_failures = np.ones(num_arms)\n",
    "        self.regrets = np.zeros(horizon)\n",
    "        self.best_arm = None\n",
    "\n",
    "    def give_best_arm(self):\n",
    "        return self.best_arm\n",
    "\n",
    "    def select_arm(self):\n",
    "        samples = np.random.beta(self.arm_successes + 1, self.arm_failures + 1)\n",
    "        return np.argmax(samples)\n",
    "\n",
    "    def run_algorithm(self, bandit):\n",
    "        for t in range(self.horizon):\n",
    "            arm_to_pull = self.select_arm()\n",
    "            reward = bandit.pull(arm_to_pull)\n",
    "            if reward == 1:\n",
    "                self.arm_successes[arm_to_pull] += 1\n",
    "            else:\n",
    "                self.arm_failures[arm_to_pull] += 1\n",
    "            self.timestep += 1\n",
    "            self.regrets[t] = bandit.regret()  \n",
    "        self.best_arm = np.argmax(self.arm_successes + self.arm_failures)\n",
    "    \n",
    "    def plot(self, label):\n",
    "        plt.plot(np.cumsum(self.regrets), label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba4ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(bandit, algorithms, horizon):\n",
    "    for algorithm in algorithms:\n",
    "        bandit = MultiBandit()\n",
    "        algorithm.run_algorithm(bandit)\n",
    "        total_regret = bandit.regret()\n",
    "        best_arm = algorithm.give_best_arm()\n",
    "        print(f\"Total Regret after {horizon} timesteps: {total_regret} with assumed best arm {best_arm}\")\n",
    "        algorithm.plot(label=f'{algorithm.__class__.__name__}')\n",
    "\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.ylabel(\"Total Regret\")\n",
    "    plt.title(\"Comparison of Algorithms\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f47bd7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "bandit = MultiBandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bca9f6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "H = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf11f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_greedy = epsilongreedyalgorithm(num_arms=bandit.num_arms(), horizon=H, epsilon=0.2)\n",
    "ucb_algorithm = ucbalgorithm(num_arms=bandit.num_arms(), horizon=H, c=0.18)\n",
    "thompson_sampling = thompsonsamplingalgorithm(num_arms=bandit.num_arms(), horizon=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e70669b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "algorithms_to_run = [epsilon_greedy, ucb_algorithm, thompson_sampling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(bandit, algorithms_to_run, H)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
