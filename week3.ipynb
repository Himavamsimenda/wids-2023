{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c391f278",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1dc861",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, file_path):\n",
    "        self.read_mdp_file(file_path)\n",
    "\n",
    "    def read_mdp_file(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        self.num_states = int(lines[0].split()[1])\n",
    "        self.num_actions = int(lines[1].split()[1])\n",
    "        self.end_states = lines[2].split()[1:]\n",
    "        self.probabilities = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "        self.rewards = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "\n",
    "        for line in lines[3:-2]:\n",
    "            s1 = int(line.split()[1])\n",
    "            a = int(line.split()[2])\n",
    "            s2 = int(line.split()[3])\n",
    "            self.probabilities[s1][a][s2] = float(line.split()[5])\n",
    "            self.rewards[s1][a][s2] = float(line.split()[4])\n",
    "\n",
    "        self.mdptype = lines[-2].split()[1]\n",
    "        self.discount = float(lines[-1].split()[1])\n",
    "\n",
    "    def solve_continuous_mdp(self):\n",
    "        val_func = np.zeros(self.num_states)\n",
    "        epsilon = 1e-6\n",
    "        while True:\n",
    "            difference = 0\n",
    "            val_func2 = val_func.copy()\n",
    "\n",
    "            for s in range(self.num_states):\n",
    "                actions = []\n",
    "                for a in range(self.num_actions):\n",
    "                    E_a = sum(\n",
    "                        self.probabilities[s][a][s2] * (self.rewards[s][a][s2] + self.discount * val_func[s2])\n",
    "                        for s2 in range(self.num_states)\n",
    "                    )\n",
    "                    actions.append(E_a)\n",
    "                 #assign the action to the corresponding state which returns highest value \n",
    "                val_func2[s] = max(actions)\n",
    "                difference = max(difference, abs(val_func[s] - val_func2[s]))\n",
    "\n",
    "            val_func = val_func2\n",
    "\n",
    "            if difference < epsilon:\n",
    "                break\n",
    "\n",
    "        policy = np.argmax(self.compute_q_values(val_func), axis=1)\n",
    "        self.write_output(val_func, policy)\n",
    "\n",
    "    def solve_episodic_mdp(self):\n",
    "        val_func = np.zeros(self.num_states)\n",
    "        epsilon = 1e-6\n",
    "\n",
    "        while True:\n",
    "            difference = 0\n",
    "            val_func2 = val_func.copy()\n",
    "\n",
    "            for s in range(self.num_states):\n",
    "                if s in self.end_states:\n",
    "                    val_func2[s] = 0\n",
    "                else:\n",
    "                    actions = []\n",
    "                    for a in range(self.num_actions):\n",
    "                        E_a = sum(\n",
    "                            self.probabilities[s][a][s2] * (self.rewards[s][a][s2] + self.discount * val_func[s2])\n",
    "                            for s2 in range(self.num_states)\n",
    "                        )\n",
    "                        actions.append(E_a)\n",
    "                    #assign the action to the corresponding state which returns highest value \n",
    "                    val_func2[s] = max(actions)\n",
    "                    difference = max(difference, abs(val_func[s] - val_func2[s]))\n",
    "\n",
    "            val_func = val_func2\n",
    "\n",
    "            if difference < epsilon:\n",
    "                break\n",
    "\n",
    "        policy = np.argmax(self.compute_q_values(val_func), axis=1)\n",
    "        self.write_output(val_func, policy)\n",
    "\n",
    "    def compute_q_values(self, val_func):\n",
    "        q_values = np.zeros((self.num_states, self.num_actions))\n",
    "        for s in range(self.num_states):\n",
    "            for a in range(self.num_actions):\n",
    "                q_values[s][a] = sum(\n",
    "                    self.probabilities[s][a][s2] * (self.rewards[s][a][s2] + self.discount * val_func[s2])\n",
    "                    for s2 in range(self.num_states)\n",
    "                )\n",
    "        return q_values\n",
    "\n",
    "    def write_output(self, val_func, policy):\n",
    "        output_file = f\"output-{self.mdptype}-mdp-{self.num_states}-{self.num_actions}.txt\"\n",
    "        with open(output_file, 'w') as outfile:\n",
    "            for i in range(self.num_states):\n",
    "                outfile.write(f\"{np.round(val_func[i], 6)} {policy[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efda349",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_files = [\"continuing-mdp-50-20.txt\", \"episodic-mdp-50-20.txt\",\"continuing-mdp-2-2.txt\"]\n",
    "    for input_file in input_files:\n",
    "        mdp = MDP(input_file)\n",
    "        mdp.read_mdp_file(input_file)\n",
    "\n",
    "        # Call appropriate solving function based on mdptype\n",
    "        if mdp.mdptype == \"continuing\":\n",
    "            mdp.solve_continuous_mdp()\n",
    "        else:\n",
    "            mdp.solve_episodic_mdp()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
