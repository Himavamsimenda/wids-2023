{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0675e128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(with_conv=False):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_shape=(2,) + env.observation_space.shape, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(64, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(16, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(env.action_space.n, kernel_initializer='uniform', activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55ebcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb15593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_and_learn(model):\n",
    "    # starting observation\n",
    "    observation = env.reset()\n",
    "    #print(observation)\n",
    "    obs = np.expand_dims(observation[0], axis=0)\n",
    "    state = np.stack((obs, obs), axis=1)\n",
    "    done = False\n",
    "\n",
    "    # observe for a set amount of timesteps and add the observations to memory\n",
    "    # uses epsilon-greedy with epsilon annealed over time\n",
    "    for t in range(observetime):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            Q = model.predict(state)\n",
    "            action = np.argmax(Q)\n",
    "            # print(env.step(action))\n",
    "        result=env.step(action)\n",
    "        observation_new, reward, done, info = result[:4]\n",
    "        #print(observation_new)\n",
    "        obs_new = np.expand_dims(observation_new, axis=0)\n",
    "        # print(state)\n",
    "        # print(obs_new)\n",
    "        state_new = np.append(np.expand_dims(obs_new, axis=0), state[:, :1, :], axis=1)\n",
    "        D.append((state, action, reward, state_new, done))\n",
    "        state = state_new\n",
    "        if done:\n",
    "            env.reset()\n",
    "            obs = np.expand_dims(observation[0], axis=0)\n",
    "            state = np.stack((obs, obs), axis=1)\n",
    "    # finish observation\n",
    "\n",
    "    # train the model with a preset minibatch size\n",
    "    # model is trained with SARSA (state action reward state action) algorithm, with adam optimizer\n",
    "    minibatch = random.sample(D, mb_size)\n",
    "\n",
    "    inputs_shape = (mb_size,) + state.shape[1:]\n",
    "    inputs = np.zeros(inputs_shape)\n",
    "    targets = np.zeros((mb_size, env.action_space.n))\n",
    "\n",
    "    for i in range(mb_size):\n",
    "        state = minibatch[i][0]\n",
    "        action = minibatch[i][1]\n",
    "        reward = minibatch[i][2]\n",
    "        state_new = minibatch[i][3]\n",
    "        done = minibatch[i][4]\n",
    "\n",
    "        inputs[i:i + 1] = np.expand_dims(state, axis=0)\n",
    "        targets[i] = model.predict(state)\n",
    "        Q_sa = model.predict(state_new)\n",
    "        \n",
    "        if done:\n",
    "            targets[i, action] = reward\n",
    "        else:\n",
    "            targets[i, action] = reward + gamma * np.max(Q_sa)\n",
    "\n",
    "        loss = model.train_on_batch(inputs, targets)\n",
    "        sys.stdout.write(\"\\rEpisode = %s,Loss = %.5f\" % (str(episode), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d134b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(model):\n",
    "    observation = env.reset()\n",
    "    obs = np.expand_dims(observation[0], axis=0)\n",
    "    state = np.stack((obs, obs), axis=1)\n",
    "    done = False\n",
    "    tot_reward = 0.0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        Q = model.predict(state)\n",
    "        action = np.argmax(Q)\n",
    "        result1=env.step(action)\n",
    "        observation, reward, done, info = result1[:4]\n",
    "        obs = np.expand_dims(observation, axis=0)\n",
    "        state = np.append(np.expand_dims(obs, axis=0), state[:, :1, :], axis=1)\n",
    "        tot_reward += reward\n",
    "    print('Game ended! Total reward: {}'.format(tot_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe22856",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    episode = 0\n",
    "    env = gym.make('CartPole-v1')\n",
    "\n",
    "    D = []\n",
    "    observetime = 500\n",
    "    epsilon = 0.95\n",
    "    gamma = 0.9\n",
    "    mb_size = 50\n",
    "    model = build_model()\n",
    "    mode = input(\"Input mode: \")\n",
    "    if mode.upper() == 'train':\n",
    "        for i in range(10):\n",
    "            for j in range(20):\n",
    "                observe_and_learn(model) \n",
    "                episode += 1\n",
    "                print('\\n')\n",
    "                D = []\n",
    "            epsilon *= 0.9\n",
    "            print('\\n')\n",
    "            play(model)\n",
    "        model_json = model.to_json()\n",
    "        with open(\"modeldetails.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        model.save_weights(\"modeldetails.h5\")\n",
    "        print(\"Saved the model\")\n",
    "    elif mode.upper() == \"playgame\":\n",
    "        model = build_model()\n",
    "        model.load_weights('modeldetails.h5')\n",
    "        for i in range(20):\n",
    "            play(model)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
